{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Analysis for Event Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "best_values_dt =  {}\n",
    "best_values_lr =  {}\n",
    "best_values_svm =  {}\n",
    "best_values_nn =  {}\n",
    "best_values_rf =  {}\n",
    "best_values_xgb =  {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Road Traffic Fine Management Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e848a405ef8430e8f1b04422b9b623d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/150370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "# source log: https://data.4tu.nl/articles/dataset/Road_Traffic_Fine_Management_Process/12683249/1\n",
    "log_all_traffic = pm4py.read_xes('Road_Traffic_Fine_Management_Process.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2013-06-18 00:00:00+02:00\n",
      "First Timestamp: 2000-01-01 00:00:00+01:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp_traffic = max(event[\"time:timestamp\"] for trace in log_all_traffic for event in trace) \n",
    "print(\"Last Timestamp:\", last_timestamp_traffic)\n",
    "first_timestamp_traffic = min(event[\"time:timestamp\"] for trace in log_all_traffic for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp_traffic) \n",
    "\n",
    "time_filtered_log_traffic = pm4py.filter_time_range(log_all_traffic, \"2012-01-01 00:00:00\", \"2013-06-18 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Fine', 'Send Fine', 'Insert Fine Notification', 'Add penalty', 'Send for Credit Collection', 'Payment', 'Insert Date Appeal to Prefecture', 'Send Appeal to Prefecture', 'Receive Result Appeal from Prefecture', 'Appeal to Judge', 'Notify Result Appeal to Offender']\n",
      "Number of different trace variants:  40\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log_traffic,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log_traffic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Fine': 5558}\n",
      "{'Send Fine': 1351, 'Send for Credit Collection': 169, 'Payment': 3719, 'Receive Result Appeal from Prefecture': 10, 'Send Appeal to Prefecture': 283, 'Notify Result Appeal to Offender': 26}\n"
     ]
    }
   ],
   "source": [
    "log_start_traffic = pm4py.get_start_activities(time_filtered_log_traffic)\n",
    "print(log_start_traffic)\n",
    "\n",
    "end_activities_traffic = pm4py.get_end_activities(time_filtered_log_traffic)\n",
    "print(end_activities_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3b922d92594a0bbaacf5be79f9c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log_top_k_traffic = pm4py.filter_variants_top_k(time_filtered_log_traffic, 10)\n",
    "pm4py.write_xes(log_top_k_traffic, \"Road_Traffic_Fine_Management_Process_filtered.xes\")\n",
    "len(log_top_k_traffic) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b1987356fd43308d9fedc5ebcb7ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_traffic = pm4py.read_xes(\"Road_Traffic_Fine_Management_Process_filtered.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_attributes_traffic = [\"amount\", \"expense\", \"totalPaymentAmount\", \"points\"]\n",
    "for attr in numeric_attributes_traffic:\n",
    "    for trace in log_traffic:\n",
    "        for event in trace:\n",
    "            try:\n",
    "                event[f'{attr}_num'] = float(event[attr])\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net_traffic, im_traffic, fm_traffic = get_petri_net(log_traffic, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs_traffic = list(pm4py.get_event_attributes(log_traffic))\n",
    "trace_attrs_traffic = list(pm4py.get_trace_attributes(log_traffic))\n",
    "event_attrs_traffic = [attr for attr in event_attrs_traffic if max(list(pm4py.get_event_attribute_values(log_traffic, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs_traffic.remove(\"time:timestamp\")\n",
    "event_attrs_traffic.remove(\"org:resource\")\n",
    "for attr in numeric_attributes_traffic:\n",
    "    event_attrs_traffic.remove(attr)\n",
    "trace_attrs_traffic = [attr for attr in trace_attrs_traffic if max(list(pm4py.get_trace_attribute_values(log_traffic, attr).values())) != 1 and \"ID\" not in attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map_traffic = extract_all_datasets(log= log_traffic, net=net_traffic, initial_marking=im_traffic, final_marking=fm_traffic, \n",
    "                                      event_level_attributes = event_attrs_traffic,\n",
    "                                      case_level_attributes=trace_attrs_traffic)\n",
    "\n",
    "# decision points \n",
    "dps_traffic = list(dp_dataset_map_traffic.keys())\n",
    "total_size_traffic = sum(len(dp_dataset_map_traffic[key]) for key in dps_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8089\t {'min_impurity_decrease': 0}\n",
      "p_3:\t0.5834\t {'min_impurity_decrease': 0.01}\n",
      "p_12:\t0.9322\t {'min_impurity_decrease': 0.1}\n",
      "p_14:\t0.9486\t {'min_impurity_decrease': 0}\n",
      "p_15:\t1.0\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.9972\t {'min_impurity_decrease': 0.01}\n",
      "p_6:\t0.9408\t {'min_impurity_decrease': 0.01}\n",
      "p_7:\t1.0\t {'min_impurity_decrease': 0}\n",
      "p_9:\t0.9563\t {'min_impurity_decrease': 0.01}\n",
      "final value: {'min_impurity_decrease': 0.022316062176165805}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_dt_traffic = {}\n",
    "\n",
    "for dp in dps_traffic:\n",
    "    X_train, y_train = [], []\n",
    "    dp_dataset = dp_dataset_map_traffic[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values_dt_traffic[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map_traffic[key]) for key,val in best_values_dt_traffic.items())/total_size_traffic for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"Road Traffic Fine Management Process\"] = best_values_dt_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8077\t {'hidden_layer_sizes': (5, 5)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_3:\t0.6091\t {'hidden_layer_sizes': (10, 10)}\n",
      "p_12:\t0.8914\t {'hidden_layer_sizes': (5,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_14:\t0.9486\t {'hidden_layer_sizes': (5,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_15:\t1.0\t {'hidden_layer_sizes': (5, 5)}\n",
      "p_4:\t0.9972\t {'hidden_layer_sizes': (5,)}\n",
      "p_6:\t0.9484\t {'hidden_layer_sizes': (10, 10)}\n",
      "p_7:\t1.0\t {'hidden_layer_sizes': (10, 10)}\n",
      "p_9:\t0.9532\t {'hidden_layer_sizes': (10, 10)}\n",
      "final value: {'hidden_layer_sizes': (10, 10)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10))}\n",
    "\n",
    "best_values_nn_traffic = {}\n",
    "max_ds_size_traffic = -1\n",
    "max_ds_key_traffic = None\n",
    "\n",
    "for dp in dps_traffic:\n",
    "    dp_dataset = dp_dataset_map_traffic[dp]\n",
    "    if len(dp_dataset) > max_ds_size_traffic: max_ds_key_traffic = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values_nn_traffic[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values_nn_traffic[max_ds_key_traffic][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"Road Traffic Fine Management Process\"] = best_values_nn_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.4969\t {'C': 0.1, 'tol': 0.001}\n",
      "p_3:\t0.5448\t {'C': 0.5, 'tol': 0.001}\n",
      "p_12:\t0.8904\t {'C': 0.1, 'tol': 0.001}\n",
      "p_14:\t0.9486\t {'C': 0.1, 'tol': 0.001}\n",
      "p_15:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "p_4:\t0.9972\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.9448\t {'C': 0.1, 'tol': 0.001}\n",
      "p_7:\t1.0\t {'C': 0.25, 'tol': 0.001}\n",
      "p_9:\t0.9563\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.19298126743722596, 'tol': 0.0009999999999999998}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_lr_traffic = {}\n",
    "\n",
    "for dp in dps_traffic:\n",
    "    dp_dataset = dp_dataset_map_traffic[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values_lr_traffic[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map_traffic[key]) for key,val in best_values_lr_traffic.items())/total_size_traffic for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"Road Traffic Fine Management Process\"] = best_values_lr_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.4969\t {'C': 0.1, 'tol': 0.001}\n",
      "p_3:\t0.5448\t {'C': 0.1, 'tol': 0.001}\n",
      "p_12:\t0.8904\t {'C': 0.1, 'tol': 0.001}\n",
      "p_14:\t0.9486\t {'C': 0.1, 'tol': 0.001}\n",
      "p_15:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "p_4:\t0.9972\t {'C': 0.1, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_6:\t0.9469\t {'C': 0.5, 'tol': 0.0005}\n",
      "p_7:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "p_9:\t0.9563\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.18744519728975692, 'tol': 0.0008906935033878038}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_svm_traffic = {}\n",
    "\n",
    "for dp in dps_traffic:\n",
    "    dp_dataset = dp_dataset_map_traffic[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values_svm_traffic[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map_traffic[key]) for key,val in best_values_svm_traffic.items())/total_size_traffic for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"Road Traffic Fine Management Process\"] = best_values_svm_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8081\t {'max_depth': 2, 'n_estimators': 100}\n",
      "p_3:\t0.5568\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_12:\t0.8904\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_14:\t0.9486\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_15:\t1.0\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_4:\t0.9972\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_6:\t0.9474\t {'max_depth': 1, 'n_estimators': 100}\n",
      "p_7:\t1.0\t {'max_depth': 1, 'n_estimators': 100}\n",
      "p_9:\t0.9563\t {'max_depth': 1, 'n_estimators': 20}\n",
      "final value: {'max_depth': 1, 'n_estimators': 48}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values_xgb_traffic = {}\n",
    "\n",
    "for dp in dps_traffic:\n",
    "    dp_dataset = dp_dataset_map_traffic[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values_xgb_traffic[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map_traffic[key]) for key,val in best_values_xgb_traffic.items())/total_size_traffic) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"Road Traffic Fine Management Process\"] = best_values_xgb_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8089\t {'max_depth': 4, 'min_impurity_decrease': 0}\n",
      "p_3:\t0.5789\t {'max_depth': 3, 'min_impurity_decrease': 0.01}\n",
      "p_12:\t0.9322\t {'max_depth': 1, 'min_impurity_decrease': 0.1}\n",
      "p_14:\t0.9486\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_15:\t1.0\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_4:\t0.9972\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_6:\t0.944\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_7:\t1.0\t {'max_depth': 2, 'min_impurity_decrease': 0}\n",
      "p_9:\t0.9563\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 3, 'min_impurity_decrease': 0.017574730968513353}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_rf_traffic = {}\n",
    "\n",
    "for dp in dps_traffic:\n",
    "    dp_dataset = dp_dataset_map_traffic[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values_rf_traffic[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map_traffic[key]) for key,val in best_values_rf_traffic.items())/total_size_traffic for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"Road Traffic Fine Management Process\"] = best_values_rf_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2012 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a379cad428d74d11bfaca0fd410a4659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "# source log: https://data.4tu.nl/articles/dataset/BPI_Challenge_2012/12689204/1\n",
    "log_all_bpi_2012 = pm4py.read_xes('BPI_Challenge_2012.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED', 'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads', 'W_Beoordelen fraude', 'W_Completeren aanvraag', 'W_Nabellen incomplete dossiers', 'W_Nabellen offertes', 'W_Valideren aanvraag', 'W_Wijzigen contractgegevens']\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {sorted(list(pm4py.get_event_attribute_values(log_all_bpi_2012,\"concept:name\").keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at subtraces of activities starting with an 'A'.\n",
    "log_bpi_a_2012 = pm4py.filter_event_attribute_values(\n",
    "    log_all_bpi_2012,\n",
    "    \"concept:name\",\n",
    "    ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED'],\n",
    "    level=\"event\",\n",
    "    retain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494c84f5e39345e598b0d2675988f1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm4py.write_xes(log_bpi_a_2012, \"BPI_Challenge_2012_only_A.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c59b4f1611445a8a813733da312d9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_bpi_2012 = pm4py.read_xes('BPI_Challenge_2012_only_A.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trace in log_all_bpi_2012:\n",
    "    trace.attributes['AMOUNT_REQ_NUM'] = float(trace.attributes['AMOUNT_REQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net_bpi_2012, im_bpi_2012, fm_bpi_2012 = get_petri_net(log_bpi_2012, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs_bpi_2012 = list(pm4py.get_event_attributes(log_bpi_2012))\n",
    "trace_attrs_bpi_2012 = list(pm4py.get_trace_attributes(log_bpi_2012))\n",
    "trace_attrs_bpi_2012.remove(\"REG_DATE\")\n",
    "trace_attrs_bpi_2012.remove(\"AMOUNT_REQ\")\n",
    "event_attrs_bpi_2012 = [attr for attr in event_attrs_bpi_2012 if max(list(pm4py.get_event_attribute_values(log_bpi_2012, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs_bpi_2012.remove(\"time:timestamp\")\n",
    "event_attrs_bpi_2012.remove(\"concept:name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map_bpi_2012 = extract_all_datasets(log= log_bpi_2012, net=net_bpi_2012, initial_marking=im_bpi_2012, final_marking=fm_bpi_2012, \n",
    "                                      event_level_attributes = event_attrs_bpi_2012,\n",
    "                                      case_level_attributes=trace_attrs_bpi_2012)\n",
    "\n",
    "# decision points \n",
    "dps_bpi_2012 = list(dp_dataset_map_bpi_2012.keys())\n",
    "total_size_bpi_2012 = sum(len(dp_dataset_map_bpi_2012[key]) for key in dps_bpi_2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8291\t {'min_impurity_decrease': 0.01}\n",
      "p_6:\t0.9925\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.4055\t {'min_impurity_decrease': 0}\n",
      "p_7:\t0.7109\t {'min_impurity_decrease': 0}\n",
      "final value: {'min_impurity_decrease': 0.0025}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_dt_bpi_2012 = {}\n",
    "\n",
    "for dp in dps_bpi_2012:\n",
    "    dp_dataset = dp_dataset_map_bpi_2012[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values_dt_bpi_2012[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2012[key]) for key,val in best_values_dt_bpi_2012.items())/total_size_bpi_2012 for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2012\"] = best_values_dt_bpi_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.826\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.9925\t {'C': 0.1, 'tol': 0.001}\n",
      "p_4:\t0.4055\t {'C': 0.1, 'tol': 0.001}\n",
      "p_7:\t0.7127\t {'C': 0.5, 'tol': 0.001}\n",
      "final value: {'C': 0.2, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_svm_bpi_2012 = {}\n",
    "\n",
    "for dp in dps_bpi_2012:\n",
    "    dp_dataset = dp_dataset_map_bpi_2012[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "\n",
    "    best_values_svm_bpi_2012[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2012[key]) for key,val in best_values_svm_bpi_2012.items())/total_size_bpi_2012 for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2012\"] = best_values_svm_bpi_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8261\t {'hidden_layer_sizes': (5, 5)}\n",
      "p_6:\t0.9925\t {'hidden_layer_sizes': (5,)}\n",
      "p_4:\t0.4055\t {'hidden_layer_sizes': (5,)}\n",
      "p_7:\t0.7138\t {'hidden_layer_sizes': (10, 10)}\n",
      "final value: {'hidden_layer_sizes': (10, 10)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10))}\n",
    "\n",
    "best_values_nn_bpi_2012 = {}\n",
    "max_ds_size_bpi_2012 = -1\n",
    "max_ds_key_bpi_2012 = None\n",
    "\n",
    "for dp in dps_bpi_2012:\n",
    "    dp_dataset = dp_dataset_map_bpi_2012[dp]\n",
    "    if len(dp_dataset) > max_ds_size_bpi_2012: max_ds_key_bpi_2012 = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values_nn_bpi_2012[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values_nn_bpi_2012[max_ds_key_bpi_2012][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2012\"] = best_values_nn_bpi_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8265\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.9925\t {'C': 0.1, 'tol': 0.001}\n",
      "p_4:\t0.4055\t {'C': 0.1, 'tol': 0.001}\n",
      "p_7:\t0.7106\t {'C': 0.5, 'tol': 0.001}\n",
      "final value: {'C': 0.2, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_lr_bpi_2012 = {}\n",
    "\n",
    "for dp in dps_bpi_2012:\n",
    "    dp_dataset = dp_dataset_map_bpi_2012[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values_lr_bpi_2012[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2012[key]) for key,val in best_values_lr_bpi_2012.items())/total_size_bpi_2012 for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2012\"] = best_values_lr_bpi_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8291\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_6:\t0.9925\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_4:\t0.4055\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_7:\t0.7118\t {'max_depth': 6, 'n_estimators': 100}\n",
      "final value: {'max_depth': 2, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values_xgb_bpi_2012 = {}\n",
    "\n",
    "for dp in dps_bpi_2012:\n",
    "    dp_dataset = dp_dataset_map_bpi_2012[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values_xgb_bpi_2012[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map_bpi_2012[key]) for key,val in best_values_xgb_bpi_2012.items())/total_size_bpi_2012) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2012\"] = best_values_xgb_bpi_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8291\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_6:\t0.9925\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_4:\t0.4055\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_7:\t0.7072\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 2, 'min_impurity_decrease': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_rf_bpi_2012 = {}\n",
    "\n",
    "for dp in dps_bpi_2012:\n",
    "    dp_dataset = dp_dataset_map_bpi_2012[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values_rf_bpi_2012[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2012[key]) for key,val in best_values_rf_bpi_2012.items())/total_size_bpi_2012 for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2012\"] = best_values_rf_bpi_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2c7969de014351bc4592f817ee7b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/251734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "# source log: https://data.4tu.nl/articles/dataset/BPI_Challenge_2019/12715853/1\n",
    "log_all_bpi_2019 = pm4py.read_xes('BPI_Challenge_2019.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2020-04-09 21:59:00+00:00\n",
      "First Timestamp: 1948-01-26 22:59:00+00:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp_bpi_2019 = max(event[\"time:timestamp\"] for trace in log_all_bpi_2019 for event in trace)\n",
    "print(\"Last Timestamp:\", last_timestamp_bpi_2019)\n",
    "first_timestamp_bpi_2019 = min(event[\"time:timestamp\"] for trace in log_all_bpi_2019 for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp_bpi_2019)\n",
    "\n",
    "time_filtered_log_bpi_2019 = pm4py.filter_time_range(log_all_bpi_2019, \"2018-09-01 00:00:00\", \"2019-01-01 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Purchase Order Item', 'Delete Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', 'Clear Invoice', 'Remove Payment Block', 'Cancel Goods Receipt', 'Change Quantity', 'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Price', 'Receive Order Confirmation', 'Change Storage Location', 'Change Delivery Indicator', 'Block Purchase Order Item', 'Create Purchase Requisition Item', 'Reactivate Purchase Order Item', 'Update Order Confirmation', 'Record Service Entry Sheet', 'SRM: Created', 'SRM: Complete', 'SRM: Awaiting Approval', 'SRM: Document Completed', 'SRM: Ordered', 'SRM: In Transfer to Execution Syst.', 'SRM: Change was Transmitted', 'SRM: Deleted', 'SRM: Transaction Completed', 'Cancel Subsequent Invoice', 'Change Approval for Purchase Order', 'Set Payment Block', 'Release Purchase Order', 'Record Subsequent Invoice', 'Change payment term', 'Change Final Invoice Indicator', 'Release Purchase Requisition']\n",
      "Number of different trace variants:  2481\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log_bpi_2019,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log_bpi_2019)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different trace variants - subtraces:  2408\n"
     ]
    }
   ],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "time_filtered_log_sub_bpi_2019 = pm4py.filter_event_attribute_values(\n",
    "    time_filtered_log_bpi_2019,\n",
    "    \"concept:name\",\n",
    "    ['Create Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', \n",
    "    'Clear Invoice', 'Record Service Entry Sheet', 'Cancel Goods Receipt', \n",
    "    'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Delivery Indicator', 'Remove Payment Block', \n",
    "    'Change Price', 'Delete Purchase Order Item', 'Change Quantity', \n",
    "    'Change Final Invoice Indicator', 'Receive Order Confirmation', 'Cancel Subsequent Invoice', \n",
    "    'Reactivate Purchase Order Item', 'Update Order Confirmation', 'Block Purchase Order Item', \n",
    "    'Change Approval for Purchase Order', 'Release Purchase Order', 'Record Subsequent Invoice', 'Set Payment Block', \n",
    "    'Create Purchase Requisition Item', 'Change Storage Location', 'Change Currency', 'Change payment term', \n",
    "    'Change Rejection Indicator', 'Release Purchase Requisition'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(time_filtered_log_sub_bpi_2019)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Purchase Order Item': 19257, 'Vendor creates invoice': 464, 'Vendor creates debit memo': 5, 'Create Purchase Requisition Item': 26600, 'Release Purchase Order': 9, 'Change Approval for Purchase Order': 28}\n",
      "Number of different trace variants - subtraces:  1382\n",
      "{'Delete Purchase Order Item': 838, 'Create Purchase Order Item': 2482, 'Clear Invoice': 4535, 'Cancel Goods Receipt': 294, 'Record Goods Receipt': 3248, 'Record Invoice Receipt': 4915, 'Cancel Invoice Receipt': 32, 'Receive Order Confirmation': 103, 'Change Delivery Indicator': 36, 'Block Purchase Order Item': 17, 'Vendor creates invoice': 15, 'Remove Payment Block': 1184, 'Record Service Entry Sheet': 545, 'Change Price': 225, 'Change Approval for Purchase Order': 652, 'Change Quantity': 116, 'Change Storage Location': 14, 'Change payment term': 1, 'Cancel Subsequent Invoice': 4, 'Update Order Confirmation': 1}\n",
      "Number of different trace variants - subtraces:  311\n"
     ]
    }
   ],
   "source": [
    "log_start_bpi_2019 = pm4py.get_start_activities(time_filtered_log_sub_bpi_2019)\n",
    "print(log_start_bpi_2019)\n",
    "filtered_log_start_bpi_2019 = pm4py.filter_start_activities(time_filtered_log_sub_bpi_2019, 'Create Purchase Order Item')\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log_start_bpi_2019)))\n",
    "\n",
    "end_activities_bpi_2019 = pm4py.get_end_activities(filtered_log_start_bpi_2019)\n",
    "print(end_activities_bpi_2019)\n",
    "filtered_log_bpi_2019 = pm4py.filter_end_activities(filtered_log_start_bpi_2019, [\"Clear Invoice\"])\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log_bpi_2019)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ddc328602f4a289d8b31620a8fae61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/3928 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "filtered_log_top_k_bpi_2019 = pm4py.filter_variants_top_k(filtered_log_bpi_2019, 10)\n",
    "\n",
    "pm4py.write_xes(filtered_log_top_k_bpi_2019, \"BPI_Challenge_2019_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea29b6403b9246eba6d91198c3fef96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/3928 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_bpi_2019 = pm4py.read_xes(\"BPI_Challenge_2019_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net_bpi_2019, im_bpi_2019, fm_bpi_2019 = get_petri_net(log_bpi_2019, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs_bpi_2019 = list(pm4py.get_event_attributes(log_bpi_2019))\n",
    "trace_attrs_bpi_2019 = list(pm4py.get_trace_attributes(log_bpi_2019))\n",
    "event_attrs_bpi_2019 = [attr for attr in event_attrs_bpi_2019 if max(list(pm4py.get_event_attribute_values(log_bpi_2019, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs_bpi_2019.remove(\"time:timestamp\")\n",
    "event_attrs_bpi_2019.remove(\"org:resource\")\n",
    "event_attrs_bpi_2019.remove(\"User\")\n",
    "trace_attrs_bpi_2019 = [attr for attr in trace_attrs_bpi_2019 if max(list(pm4py.get_trace_attribute_values(log_bpi_2019, attr).values())) != 1 and \"ID\" not in attr]\n",
    "trace_attrs_bpi_2019.remove(\"Name\")\n",
    "trace_attrs_bpi_2019.remove(\"Item\")\n",
    "trace_attrs_bpi_2019.remove(\"Purchasing Document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map_bpi_2019 = extract_all_datasets(log= log_bpi_2019, net=net_bpi_2019, initial_marking=im_bpi_2019, final_marking=fm_bpi_2019, \n",
    "                                      event_level_attributes = event_attrs_bpi_2019,\n",
    "                                      case_level_attributes=trace_attrs_bpi_2019)\n",
    "\n",
    "# decision points \n",
    "dps_bpi_2019 = list(dp_dataset_map_bpi_2019.keys())\n",
    "total_size_bpi_2019 = sum(len(dp_dataset_map_bpi_2019[key]) for key in dps_bpi_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9948\t {'min_impurity_decrease': 0}\n",
      "p_8:\t0.8236\t {'min_impurity_decrease': 0.01}\n",
      "p_11:\t1.0\t {'min_impurity_decrease': 0}\n",
      "p_3:\t0.9517\t {'min_impurity_decrease': 0}\n",
      "final value: {'min_impurity_decrease': 0.0024920188992465842}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_dt_bpi_2019 = {}\n",
    "\n",
    "for dp in dps_bpi_2019:\n",
    "    dp_dataset = dp_dataset_map_bpi_2019[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values_dt_bpi_2019[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2019[key]) for key,val in best_values_dt_bpi_2019.items())/total_size_bpi_2019 for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2019\"] = best_values_dt_bpi_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validaion Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9995\t {'hidden_layer_sizes': (10, 10)}\n",
      "p_8:\t0.8244\t {'hidden_layer_sizes': (5, 5)}\n",
      "p_11:\t0.998\t {'hidden_layer_sizes': (5,)}\n",
      "p_3:\t0.9576\t {'hidden_layer_sizes': (5,)}\n",
      "final value: {'hidden_layer_sizes': (5,)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10))}\n",
    "\n",
    "best_values_nn_bpi_2019 = {}\n",
    "max_ds_size_bpi_2019 = -1\n",
    "max_ds_key_bpi_2019 = None\n",
    "\n",
    "for dp in dps_bpi_2019:\n",
    "    dp_dataset = dp_dataset_map_bpi_2019[dp]\n",
    "    if len(dp_dataset) > max_ds_size_bpi_2019: max_ds_key_bpi_2019 = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values_nn_bpi_2019[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values_nn_bpi_2019[max_ds_key_bpi_2019][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2019\"] = best_values_nn_bpi_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "p_8:\t0.8452\t {'C': 0.1, 'tol': 0.001}\n",
      "p_11:\t1.0\t {'C': 0.25, 'tol': 0.001}\n",
      "p_3:\t0.9616\t {'C': 0.5, 'tol': 0.001}\n",
      "final value: {'C': 0.23769952751883539, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_lr_bpi_2019 = {}\n",
    "\n",
    "for dp in dps_bpi_2019:\n",
    "    dp_dataset = dp_dataset_map_bpi_2019[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values_lr_bpi_2019[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2019[key]) for key,val in best_values_lr_bpi_2019.items())/total_size_bpi_2019 for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2019\"] = best_values_lr_bpi_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.995\t {'C': 0.1, 'tol': 0.001}\n",
      "p_8:\t0.8273\t {'C': 0.1, 'tol': 0.001}\n",
      "p_11:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "p_3:\t0.9651\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.1, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_svm_bpi_2019 = {}\n",
    "\n",
    "for dp in dps_bpi_2019:\n",
    "    dp_dataset = dp_dataset_map_bpi_2019[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values_svm_bpi_2019[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2019[key]) for key,val in best_values_svm_bpi_2019.items())/total_size_bpi_2019 for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2019\"] = best_values_svm_bpi_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9948\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_8:\t0.8415\t {'max_depth': 3, 'n_estimators': 150}\n",
      "p_11:\t1.0\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_3:\t0.965\t {'max_depth': 6, 'n_estimators': 20}\n",
      "final value: {'max_depth': 3, 'n_estimators': 52}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "# data sets contain special characters which xgb boost cannot handel\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "best_values_xgb_bpi_2019 = {}\n",
    "\n",
    "for dp in dps_bpi_2019: \n",
    "    dp_dataset = dp_dataset_map_bpi_2019[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    # remove special characters\n",
    "    # solution from: https://stackoverflow.com/questions/48645846/pythons-xgoost-valueerrorfeature-names-may-not-contain-or\n",
    "    X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values_xgb_bpi_2019[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map_bpi_2019[key]) for key,val in best_values_xgb_bpi_2019.items())/total_size_bpi_2019) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2019\"] = best_values_xgb_bpi_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9998\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_8:\t0.8101\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_11:\t1.0\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_3:\t0.9494\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 6, 'min_impurity_decrease': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_rf_bpi_2019 = {}\n",
    "\n",
    "for dp in dps_bpi_2019:\n",
    "    dp_dataset = dp_dataset_map_bpi_2019[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values_rf_bpi_2019[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2019[key]) for key,val in best_values_rf_bpi_2019.items())/total_size_bpi_2019 for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2019\"] = best_values_rf_bpi_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cb90dc053e49009ff718b0d44fb583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/31509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "# source log: https://data.4tu.nl/articles/dataset/BPI_Challenge_2017/12696884/1\n",
    "log_all_bpi_2017 = pm4py.read_xes('BPI Challenge 2017.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['A_Create Application', 'A_Submitted', 'W_Handle leads', 'W_Complete application', 'A_Concept', 'A_Accepted', 'O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'W_Call after offers', 'A_Complete', 'W_Validate application', 'A_Validating', 'O_Returned', 'W_Call incomplete files', 'A_Incomplete', 'O_Accepted', 'A_Pending', 'A_Denied', 'O_Refused', 'O_Cancelled', 'A_Cancelled', 'O_Sent (online only)', 'W_Assess potential fraud', 'W_Personal Loan collection', 'W_Shortened completion ']\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(log_all_bpi_2017,\"concept:name\").keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different trace variants - subtraces:  877\n",
      "Number of different trace variants - filtered subtraces:  10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b6d4ccaa9e4df4be4ad71b4dcd6800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/22771 [00:04<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We only look at subtraces of activities starting with an 'O'.\n",
    "# Semantically, this means we look at the events corresponding to\n",
    "# the offer of a trace.\n",
    "log_o_bpi_2017 = pm4py.filter_event_attribute_values(\n",
    "    log_all_bpi_2017,\n",
    "    \"concept:name\",\n",
    "    ['O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Returned',\n",
    "     'O_Accepted', 'O_Cancelled', 'O_Refused', 'O_Sent (online only)'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log_o_bpi_2017)))\n",
    "\n",
    "filtered_log_o_bpi_2017 = pm4py.filter_variants(log_o_bpi_2017, [\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (online only)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Refused')])\n",
    "\n",
    "print(\"Number of different trace variants - filtered subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log_o_bpi_2017)))\n",
    "\n",
    "pm4py.write_xes(filtered_log_o_bpi_2017, \"BPIChallenge2017_filtered.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb910711bf404bad8a0f1893411110e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/22771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_bpi_2017 = pm4py.read_xes('BPIChallenge2017_filtered.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net_bpi_2017, im_bpi_2017, fm_bpi_2017 = get_petri_net(log_bpi_2017, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_attrs_bpi_2017 = list(pm4py.get_trace_attributes(log_bpi_2017))\n",
    "event_attrs_bpi_2017 = list(pm4py.get_event_attributes(log_bpi_2017))\n",
    "event_attrs_bpi_2017 = [attr for attr in event_attrs_bpi_2017 if max(list(pm4py.get_event_attribute_values(log_bpi_2017, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs_bpi_2017.remove(\"time:timestamp\")\n",
    "event_attrs_bpi_2017.remove(\"org:resource\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map_bpi_2017 = extract_all_datasets(log= log_bpi_2017, net=net_bpi_2017, initial_marking=im_bpi_2017, final_marking=fm_bpi_2017, \n",
    "                                      event_level_attributes = event_attrs_bpi_2017,\n",
    "                                      case_level_attributes=trace_attrs_bpi_2017)\n",
    "\n",
    "# decision points \n",
    "dps_bpi_2017 = list(dp_dataset_map_bpi_2017.keys())\n",
    "total_size_bpi_2017 = sum(len(dp_dataset_map_bpi_2017[key]) for key in dps_bpi_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9819\t {'min_impurity_decrease': 0.01}\n",
      "p_5:\t0.5702\t {'min_impurity_decrease': 0}\n",
      "p_6:\t0.7946\t {'min_impurity_decrease': 0.01}\n",
      "final value: {'min_impurity_decrease': 0.006666666666666667}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_dt_bpi_2017 = {}\n",
    "\n",
    "for dp in dps_bpi_2017:\n",
    "    dp_dataset = dp_dataset_map_bpi_2017[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values_dt_bpi_2017[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2017[key]) for key,val in best_values_dt_bpi_2017.items())/total_size_bpi_2017 for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2017\"] = best_values_dt_bpi_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9819\t {'C': 0.1, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5502\t {'C': 0.25, 'tol': 0.001}\n",
      "p_6:\t0.7946\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.15000000000000002, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_svm_bpi_2017 = {}\n",
    "\n",
    "for dp in dps_bpi_2017:\n",
    "    dp_dataset = dp_dataset_map_bpi_2017[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values_svm_bpi_2017[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2017[key]) for key,val in best_values_svm_bpi_2017.items())/total_size_bpi_2017 for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2017\"] = best_values_svm_bpi_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9819\t {'hidden_layer_sizes': (5,)}\n",
      "p_5:\t0.551\t {'hidden_layer_sizes': (5,)}\n",
      "p_6:\t0.7947\t {'hidden_layer_sizes': (5, 5)}\n",
      "final value: {'hidden_layer_sizes': (5, 5)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10))}\n",
    "\n",
    "best_values_nn_bpi_2017 = {}\n",
    "max_ds_size_bpi_2017 = -1\n",
    "max_ds_key_bpi_2017 = None\n",
    "\n",
    "for dp in dps_bpi_2017:\n",
    "    dp_dataset = dp_dataset_map_bpi_2017[dp]\n",
    "    if len(dp_dataset) > max_ds_size_bpi_2017: max_ds_key_bpi_2017 = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values_nn_bpi_2017[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values_nn_bpi_2017[max_ds_key_bpi_2017][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2017\"] = best_values_nn_bpi_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9819\t {'C': 0.1, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5501\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.7946\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.09999999999999999, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values_lr_bpi_2017 = {}\n",
    "\n",
    "for dp in dps_bpi_2017:\n",
    "    dp_dataset = dp_dataset_map_bpi_2017[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values_lr_bpi_2017[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2017[key]) for key,val in best_values_lr_bpi_2017.items())/total_size_bpi_2017 for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2017\"] = best_values_lr_bpi_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9819\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_5:\t0.5623\t {'max_depth': 6, 'n_estimators': 150}\n",
      "p_6:\t0.7946\t {'max_depth': 6, 'n_estimators': 150}\n",
      "final value: {'max_depth': 4, 'n_estimators': 107}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values_xgb_bpi_2017 = {}\n",
    "\n",
    "for dp in dps_bpi_2017:\n",
    "    dp_dataset = dp_dataset_map_bpi_2017[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values_xgb_bpi_2017[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map_bpi_2017[key]) for key,val in best_values_xgb_bpi_2017.items())/total_size_bpi_2017) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2017\"] = best_values_xgb_bpi_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_4:\t0.9819\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_5:\t0.5504\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_6:\t0.7946\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 3, 'min_impurity_decrease': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values_rf_bpi_2017 = {}\n",
    "\n",
    "for dp in dps_bpi_2017:\n",
    "    dp_dataset = dp_dataset_map_bpi_2017[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values_rf_bpi_2017[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map_bpi_2017[key]) for key,val in best_values_rf_bpi_2017.items())/total_size_bpi_2017 for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2017\"] = best_values_rf_bpi_2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'min_impurity_decrease': 0},\n",
       "  p_3: {'min_impurity_decrease': 0.01},\n",
       "  p_12: {'min_impurity_decrease': 0.1},\n",
       "  p_14: {'min_impurity_decrease': 0},\n",
       "  p_15: {'min_impurity_decrease': 0},\n",
       "  p_4: {'min_impurity_decrease': 0.01},\n",
       "  p_6: {'min_impurity_decrease': 0.01},\n",
       "  p_7: {'min_impurity_decrease': 0},\n",
       "  p_9: {'min_impurity_decrease': 0.01}},\n",
       " 'BPI Challenge 2012': {p_5: {'min_impurity_decrease': 0.01},\n",
       "  p_6: {'min_impurity_decrease': 0},\n",
       "  p_4: {'min_impurity_decrease': 0},\n",
       "  p_7: {'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2019': {p_4: {'min_impurity_decrease': 0},\n",
       "  p_8: {'min_impurity_decrease': 0.01},\n",
       "  p_11: {'min_impurity_decrease': 0},\n",
       "  p_3: {'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2017': {p_4: {'min_impurity_decrease': 0.01},\n",
       "  p_5: {'min_impurity_decrease': 0},\n",
       "  p_6: {'min_impurity_decrease': 0.01}}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_3: {'C': 0.5, 'tol': 0.001},\n",
       "  p_12: {'C': 0.1, 'tol': 0.001},\n",
       "  p_14: {'C': 0.1, 'tol': 0.001},\n",
       "  p_15: {'C': 0.1, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001},\n",
       "  p_7: {'C': 0.25, 'tol': 0.001},\n",
       "  p_9: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2012': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_7: {'C': 0.5, 'tol': 0.001}},\n",
       " 'BPI Challenge 2019': {p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_8: {'C': 0.1, 'tol': 0.001},\n",
       "  p_11: {'C': 0.25, 'tol': 0.001},\n",
       "  p_3: {'C': 0.5, 'tol': 0.001}},\n",
       " 'BPI Challenge 2017': {p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001}}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_3: {'C': 0.1, 'tol': 0.001},\n",
       "  p_12: {'C': 0.1, 'tol': 0.001},\n",
       "  p_14: {'C': 0.1, 'tol': 0.001},\n",
       "  p_15: {'C': 0.1, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.5, 'tol': 0.0005},\n",
       "  p_7: {'C': 0.1, 'tol': 0.001},\n",
       "  p_9: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2012': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_7: {'C': 0.5, 'tol': 0.001}},\n",
       " 'BPI Challenge 2019': {p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_8: {'C': 0.1, 'tol': 0.001},\n",
       "  p_11: {'C': 0.1, 'tol': 0.001},\n",
       "  p_3: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2017': {p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_5: {'C': 0.25, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001}}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'hidden_layer_sizes': (5, 5)},\n",
       "  p_3: {'hidden_layer_sizes': (10, 10)},\n",
       "  p_12: {'hidden_layer_sizes': (5,)},\n",
       "  p_14: {'hidden_layer_sizes': (5,)},\n",
       "  p_15: {'hidden_layer_sizes': (5, 5)},\n",
       "  p_4: {'hidden_layer_sizes': (5,)},\n",
       "  p_6: {'hidden_layer_sizes': (10, 10)},\n",
       "  p_7: {'hidden_layer_sizes': (10, 10)},\n",
       "  p_9: {'hidden_layer_sizes': (10, 10)}},\n",
       " 'BPI Challenge 2012': {p_5: {'hidden_layer_sizes': (5, 5)},\n",
       "  p_6: {'hidden_layer_sizes': (5,)},\n",
       "  p_4: {'hidden_layer_sizes': (5,)},\n",
       "  p_7: {'hidden_layer_sizes': (10, 10)}},\n",
       " 'BPI Challenge 2019': {p_4: {'hidden_layer_sizes': (10, 10)},\n",
       "  p_8: {'hidden_layer_sizes': (5, 5)},\n",
       "  p_11: {'hidden_layer_sizes': (5,)},\n",
       "  p_3: {'hidden_layer_sizes': (5,)}},\n",
       " 'BPI Challenge 2017': {p_4: {'hidden_layer_sizes': (5,)},\n",
       "  p_5: {'hidden_layer_sizes': (5,)},\n",
       "  p_6: {'hidden_layer_sizes': (5, 5)}}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'max_depth': 4,\n",
       "   'min_impurity_decrease': 0},\n",
       "  p_3: {'max_depth': 3, 'min_impurity_decrease': 0.01},\n",
       "  p_12: {'max_depth': 1, 'min_impurity_decrease': 0.1},\n",
       "  p_14: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_15: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_4: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_6: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_7: {'max_depth': 2, 'min_impurity_decrease': 0},\n",
       "  p_9: {'max_depth': 1, 'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2012': {p_5: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_6: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_4: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_7: {'max_depth': 6, 'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2019': {p_4: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_8: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_11: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_3: {'max_depth': 6, 'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2017': {p_4: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_5: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_6: {'max_depth': 1, 'min_impurity_decrease': 0}}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'max_depth': 2,\n",
       "   'n_estimators': 100},\n",
       "  p_3: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_12: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_14: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_15: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_6: {'max_depth': 1, 'n_estimators': 100},\n",
       "  p_7: {'max_depth': 1, 'n_estimators': 100},\n",
       "  p_9: {'max_depth': 1, 'n_estimators': 20}},\n",
       " 'BPI Challenge 2012': {p_5: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_6: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_7: {'max_depth': 6, 'n_estimators': 100}},\n",
       " 'BPI Challenge 2019': {p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_8: {'max_depth': 3, 'n_estimators': 150},\n",
       "  p_11: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_3: {'max_depth': 6, 'n_estimators': 20}},\n",
       " 'BPI Challenge 2017': {p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_5: {'max_depth': 6, 'n_estimators': 150},\n",
       "  p_6: {'max_depth': 6, 'n_estimators': 150}}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
