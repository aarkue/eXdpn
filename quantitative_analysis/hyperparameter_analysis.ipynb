{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Analysis for Event Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "best_values_dt =  {}\n",
    "best_values_lr =  {}\n",
    "best_values_svm =  {}\n",
    "best_values_nn =  {}\n",
    "best_values_rf =  {}\n",
    "best_values_xgb =  {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Road Traffic Fine Management Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828f35b3c5d043809867b4c39f56ee04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/150370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('Road_Traffic_Fine_Management_Process.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2013-06-18 00:00:00+02:00\n",
      "First Timestamp: 2000-01-01 00:00:00+01:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp = max(event[\"time:timestamp\"] for trace in log_all for event in trace) \n",
    "print(\"Last Timestamp:\", last_timestamp)\n",
    "first_timestamp = min(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp) \n",
    "\n",
    "time_filtered_log = pm4py.filter_time_range(log_all, \"2012-01-01 00:00:00\", \"2013-06-18 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Fine', 'Send Fine', 'Insert Fine Notification', 'Add penalty', 'Send for Credit Collection', 'Payment', 'Insert Date Appeal to Prefecture', 'Send Appeal to Prefecture', 'Receive Result Appeal from Prefecture', 'Appeal to Judge', 'Notify Result Appeal to Offender']\n",
      "Number of different trace variants:  40\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Fine': 5558}\n",
      "{'Send Fine': 1351, 'Send for Credit Collection': 169, 'Payment': 3719, 'Receive Result Appeal from Prefecture': 10, 'Send Appeal to Prefecture': 283, 'Notify Result Appeal to Offender': 26}\n"
     ]
    }
   ],
   "source": [
    "log_start = pm4py.get_start_activities(time_filtered_log)\n",
    "print(log_start)\n",
    "\n",
    "end_activities = pm4py.get_end_activities(time_filtered_log)\n",
    "print(end_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d464856e92ef4e979fb05b798def1a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log = pm4py.filter_variants_top_k(time_filtered_log, 10)\n",
    "pm4py.write_xes(log, \"Road_Traffic_Fine_Management_Process_filtered.xes\")\n",
    "len(log) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9163d97c8b34470b26063ae6348e79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes(\"Road_Traffic_Fine_Management_Process_filtered.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\")\n",
    "numeric_attributes = [\"amount\", \"expense\", \"totalPaymentAmount\", \"points\"]\n",
    "trace_attrs = [attr for attr in trace_attrs if max(list(pm4py.get_trace_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())\n",
    "total_size = sum(len(dp_dataset_map[key]) for key in dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8089\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5834\t {'min_impurity_decrease': 0.01}\n",
      "p_4:\t0.9972\t {'min_impurity_decrease': 0.01}\n",
      "p_6:\t0.9441\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'min_impurity_decrease': 0.01}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.9322\t {'min_impurity_decrease': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'min_impurity_decrease': 0}\n",
      "final value: {'min_impurity_decrease': 0.020129932243921882}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    X_train, y_train = [], []\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8077\t {'hidden_layer_sizes': (5, 5), 'learning_rate': 'adaptive'}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'hidden_layer_sizes': (10, 10), 'learning_rate': 'invscaling'}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_14:\t0.9486\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.6274\t {'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive'}\n",
      "p_4:\t0.9972\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "p_6:\t0.9483\t {'hidden_layer_sizes': (10, 10), 'learning_rate': 'invscaling'}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8943\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'hidden_layer_sizes': (10, 10), 'learning_rate': 'constant'}\n",
      "final value: {'hidden_layer_sizes': (10, 10), 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values[max_ds_key][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.4969\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'C': 0.25, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5448\t {'C': 0.5, 'tol': 0.001}\n",
      "p_4:\t0.9972\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.9448\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.19298126743722602, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.4969\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5448\t {'C': 0.1, 'tol': 0.001}\n",
      "p_4:\t0.9972\t {'C': 0.1, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_6:\t0.9448\t {'C': 0.5, 'tol': 0.001}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'C': 0.1, 'tol': 0.001}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.18744519728975692, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8081\t {'max_depth': 2, 'n_estimators': 100}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'max_depth': 1, 'n_estimators': 100}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'max_depth': 1, 'n_estimators': 20}\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5568\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_4:\t0.9972\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_6:\t0.9474\t {'max_depth': 1, 'n_estimators': 100}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'max_depth': 1, 'n_estimators': 20}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'max_depth': 1, 'n_estimators': 20}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'max_depth': 1, 'n_estimators': 20}\n",
      "final value: {'max_depth': 1, 'n_estimators': 48}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8089\t {'max_depth': 4, 'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'max_depth': 2, 'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5643\t {'max_depth': 2, 'min_impurity_decrease': 0.01}\n",
      "p_4:\t0.9972\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_6:\t0.9441\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.9322\t {'max_depth': 1, 'min_impurity_decrease': 0.05}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 3, 'min_impurity_decrease': 0.009880430450378638}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2012 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b680d7d2ce134a98a616a2400eea1305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('financial_log.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED', 'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads', 'W_Beoordelen fraude', 'W_Completeren aanvraag', 'W_Nabellen incomplete dossiers', 'W_Nabellen offertes', 'W_Valideren aanvraag', 'W_Wijzigen contractgegevens']\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {sorted(list(pm4py.get_event_attribute_values(log_all,\"concept:name\").keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at subtraces of activities starting with an 'A'.\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    log_all,\n",
    "    \"concept:name\",\n",
    "    ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED'],\n",
    "    level=\"event\",\n",
    "    retain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f223b2830734affa85d7755e77bafc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm4py.write_xes(log, \"BPI Challenge 2012 only A.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc05c0df2af450490cca94e84bed655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes('BPI Challenge 2012 only A.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "trace_attrs.remove(\"REG_DATE\")\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"concept:name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())\n",
    "total_size = sum(len(dp_dataset_map[key]) for key in dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8291\t {'min_impurity_decrease': 0.01}\n",
      "p_7:\t0.6865\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.6175\t {'min_impurity_decrease': 0.01}\n",
      "p_6:\t0.9925\t {'min_impurity_decrease': 0.01}\n",
      "final value: {'min_impurity_decrease': 0.007500000000000001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8266\t {'C': 0.5, 'tol': 0.001}\n",
      "p_7:\t0.7126\t {'C': 0.5, 'tol': 0.001}\n",
      "p_4:\t0.6177\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.9925\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.30000000000000004, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "\n",
    "    best_values[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8301\t {'hidden_layer_sizes': (5, 5), 'learning_rate': 'invscaling'}\n",
      "p_7:\t0.7109\t {'hidden_layer_sizes': (10, 10), 'learning_rate': 'invscaling'}\n",
      "p_4:\t0.6211\t {'hidden_layer_sizes': (5,), 'learning_rate': 'invscaling'}\n",
      "p_6:\t0.9925\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "final value: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values[max_ds_key][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8268\t {'C': 0.1, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_7:\t0.711\t {'C': 0.25, 'tol': 0.001}\n",
      "p_4:\t0.6207\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.9925\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.13749999999999998, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8378\t {'max_depth': 2, 'n_estimators': 20}\n",
      "p_7:\t0.7105\t {'max_depth': 2, 'n_estimators': 20}\n",
      "p_4:\t0.6182\t {'max_depth': 4, 'n_estimators': 100}\n",
      "p_6:\t0.9925\t {'max_depth': 1, 'n_estimators': 20}\n",
      "final value: {'max_depth': 2, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.834\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_7:\t0.7089\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_4:\t0.6179\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_6:\t0.9925\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 5, 'min_impurity_decrease': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4cb661bd1c4e94b5b587776e8e7776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/251734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI_Challenge_2019.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2020-04-09 21:59:00+00:00\n",
      "First Timestamp: 1948-01-26 22:59:00+00:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp = max(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"Last Timestamp:\", last_timestamp)\n",
    "first_timestamp = min(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp)\n",
    "\n",
    "time_filtered_log = pm4py.filter_time_range(log_all, \"2018-09-01 00:00:00\", \"2018-12-01 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Purchase Order Item', 'Delete Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', 'Clear Invoice', 'Remove Payment Block', 'Cancel Goods Receipt', 'Change Quantity', 'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Price', 'Receive Order Confirmation', 'Change Storage Location', 'Change Delivery Indicator', 'Block Purchase Order Item', 'Create Purchase Requisition Item', 'Reactivate Purchase Order Item', 'Record Service Entry Sheet', 'SRM: Created', 'SRM: Complete', 'SRM: Awaiting Approval', 'SRM: Document Completed', 'SRM: Ordered', 'SRM: In Transfer to Execution Syst.', 'SRM: Change was Transmitted', 'SRM: Deleted', 'SRM: Transaction Completed', 'Cancel Subsequent Invoice', 'Change Approval for Purchase Order', 'Release Purchase Order', 'Update Order Confirmation', 'Record Subsequent Invoice', 'Change payment term', 'Change Final Invoice Indicator', 'Set Payment Block']\n",
      "Number of different trace variants:  1579\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different trace variants - subtraces:  1535\n"
     ]
    }
   ],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    time_filtered_log,\n",
    "    \"concept:name\",\n",
    "    ['Create Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', \n",
    "    'Clear Invoice', 'Record Service Entry Sheet', 'Cancel Goods Receipt', \n",
    "    'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Delivery Indicator', 'Remove Payment Block', \n",
    "    'Change Price', 'Delete Purchase Order Item', 'Change Quantity', \n",
    "    'Change Final Invoice Indicator', 'Receive Order Confirmation', 'Cancel Subsequent Invoice', \n",
    "    'Reactivate Purchase Order Item', 'Update Order Confirmation', 'Block Purchase Order Item', \n",
    "    'Change Approval for Purchase Order', 'Release Purchase Order', 'Record Subsequent Invoice', 'Set Payment Block', \n",
    "    'Create Purchase Requisition Item', 'Change Storage Location', 'Change Currency', 'Change payment term', \n",
    "    'Change Rejection Indicator', 'Release Purchase Requisition'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Purchase Order Item': 11176, 'Vendor creates invoice': 347, 'Vendor creates debit memo': 3, 'Create Purchase Requisition Item': 14665, 'Release Purchase Order': 1, 'Change Approval for Purchase Order': 13}\n",
      "Number of different trace variants - subtraces:  959\n",
      "{'Delete Purchase Order Item': 1850, 'Clear Invoice': 10521, 'Create Purchase Order Item': 1521, 'Cancel Goods Receipt': 175, 'Record Goods Receipt': 4576, 'Record Invoice Receipt': 5316, 'Receive Order Confirmation': 123, 'Change Delivery Indicator': 104, 'Block Purchase Order Item': 17, 'Record Service Entry Sheet': 401, 'Change Approval for Purchase Order': 211, 'Change Quantity': 107, 'Change Storage Location': 15, 'Change Price': 48, 'Remove Payment Block': 1187, 'Release Purchase Order': 1, 'Change payment term': 1, 'Cancel Subsequent Invoice': 4, 'Cancel Invoice Receipt': 1, 'Record Subsequent Invoice': 1, 'Vendor creates invoice': 23, 'Update Order Confirmation': 1, 'Set Payment Block': 1}\n",
      "Number of different trace variants - subtraces:  506\n"
     ]
    }
   ],
   "source": [
    "log_start = pm4py.get_start_activities(log)\n",
    "print(log_start)\n",
    "filtered_log = pm4py.filter_start_activities(log, 'Create Purchase Order Item')\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))\n",
    "\n",
    "end_activities = pm4py.get_end_activities(log)\n",
    "print(end_activities)\n",
    "filtered_log = pm4py.filter_end_activities(log, [\"Clear Invoice\"])\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffcd9af971b4aeb85ed3620b8f1fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/8345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8345"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log = pm4py.filter_variants_top_k(filtered_log, 10)\n",
    "\n",
    "pm4py.write_xes(log, \"BPI_Challenge_2019_filtered_top_k.xes\")\n",
    "len(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94c290100e4462cac14669df370a48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/8345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes(\"BPI_Challenge_2019_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\")\n",
    "event_attrs.remove(\"User\")\n",
    "trace_attrs = [attr for attr in trace_attrs if max(list(pm4py.get_trace_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "trace_attrs.remove(\"Name\")\n",
    "trace_attrs.remove(\"Item\")\n",
    "trace_attrs.remove(\"Purchasing Document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())\n",
    "total_size = sum(len(dp_dataset_map[key]) for key in dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.8621\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.8753\t {'min_impurity_decrease': 0.01}\n",
      "p_9:\t0.9952\t {'min_impurity_decrease': 0.01}\n",
      "final value: {'min_impurity_decrease': 0.006666666666666667}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validaion Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.8669\t {'hidden_layer_sizes': (10, 10), 'learning_rate': 'constant'}\n",
      "p_4:\t0.8389\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "p_9:\t0.9988\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "final value: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values[max_ds_key][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.8208\t {'C': 0.5, 'tol': 0.001}\n",
      "p_4:\t0.8525\t {'C': 0.1, 'tol': 0.001}\n",
      "p_9:\t0.998\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.23333333333333334, 'tol': 0.0010000000000000002}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.8662\t {'C': 0.5, 'tol': 0.001}\n",
      "p_4:\t0.8364\t {'C': 0.1, 'tol': 0.001}\n",
      "p_9:\t0.9983\t {'C': 0.25, 'tol': 0.001}\n",
      "final value: {'C': 0.2833333333333333, 'tol': 0.0010000000000000002}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.8197\t {'max_depth': 6, 'n_estimators': 150}\n",
      "p_4:\t0.8614\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_9:\t0.996\t {'max_depth': 2, 'n_estimators': 100}\n",
      "final value: {'max_depth': 3, 'n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "# data sets contain special characters which xgb boost cannot handel\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "best_values = {}\n",
    "for dp in dps: \n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    # remove special characters\n",
    "    # solution from: https://stackoverflow.com/questions/48645846/pythons-xgoost-valueerrorfeature-names-may-not-contain-or\n",
    "    X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.6672\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "p_4:\t0.836\t {'max_depth': 3, 'min_impurity_decrease': 0}\n",
      "p_9:\t0.9962\t {'max_depth': 6, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 5, 'min_impurity_decrease': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e70bde1d73409d869b1b82bc2e1602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/31509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI Challenge 2017.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['A_Create Application', 'A_Submitted', 'W_Handle leads', 'W_Complete application', 'A_Concept', 'A_Accepted', 'O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'W_Call after offers', 'A_Complete', 'W_Validate application', 'A_Validating', 'O_Returned', 'W_Call incomplete files', 'A_Incomplete', 'O_Accepted', 'A_Pending', 'A_Denied', 'O_Refused', 'O_Cancelled', 'A_Cancelled', 'O_Sent (online only)', 'W_Assess potential fraud', 'W_Personal Loan collection', 'W_Shortened completion ']\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(log_all,\"concept:name\").keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different trace variants - subtraces:  877\n",
      "Number of different trace variants - filtered subtraces:  10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55a88e54a3b43779e8d9e6c38f631ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/22771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We only look at subtraces of activities starting with an 'O'.\n",
    "# Semantically, this means we look at the events corresponding to\n",
    "# the offer of a trace.\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    log_all,\n",
    "    \"concept:name\",\n",
    "    ['O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Returned',\n",
    "     'O_Accepted', 'O_Cancelled', 'O_Refused', 'O_Sent (online only)'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log)))\n",
    "\n",
    "filtered_log = pm4py.filter_variants(log, [\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (online only)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Refused')])\n",
    "\n",
    "print(\"Number of different trace variants - filtered subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))\n",
    "\n",
    "pm4py.write_xes(filtered_log, \"BPIChallenge2017_filtered.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faa9aec30834680ae0216fecc4aa7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/22771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes('BPIChallenge2017_filtered.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())\n",
    "total_size = sum(len(dp_dataset_map[key]) for key in dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5703\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.9819\t {'min_impurity_decrease': 0.01}\n",
      "p_6:\t0.7946\t {'min_impurity_decrease': 0.01}\n",
      "final value: {'min_impurity_decrease': 0.006666666666666667}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Liblinear failed to converge, increase the number of iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5502\t {'C': 0.25, 'tol': 0.001}\n",
      "p_4:\t0.9819\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.7946\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.15000000000000002, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5532\t {'hidden_layer_sizes': (5, 5), 'learning_rate': 'invscaling'}\n",
      "p_4:\t0.9819\t {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}\n",
      "p_6:\t0.7946\t {'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive'}\n",
      "final value: {'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values[max_ds_key][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5501\t {'C': 0.1, 'tol': 0.001}\n",
      "p_4:\t0.9819\t {'C': 0.1, 'tol': 0.001}\n",
      "p_6:\t0.7946\t {'C': 0.1, 'tol': 0.001}\n",
      "final value: {'C': 0.09999999999999999, 'tol': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5623\t {'max_depth': 6, 'n_estimators': 150}\n",
      "p_4:\t0.9819\t {'max_depth': 1, 'n_estimators': 20}\n",
      "p_6:\t0.7946\t {'max_depth': 6, 'n_estimators': 150}\n",
      "final value: {'max_depth': 4, 'n_estimators': 107}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.5501\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_4:\t0.9819\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "p_6:\t0.7946\t {'max_depth': 1, 'min_impurity_decrease': 0}\n",
      "final value: {'max_depth': 1, 'min_impurity_decrease': 0.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'min_impurity_decrease': 0},\n",
       "  p_7: {'min_impurity_decrease': 0},\n",
       "  p_14: {'min_impurity_decrease': 0},\n",
       "  p_3: {'min_impurity_decrease': 0.01},\n",
       "  p_4: {'min_impurity_decrease': 0.01},\n",
       "  p_6: {'min_impurity_decrease': 0},\n",
       "  p_9: {'min_impurity_decrease': 0.01},\n",
       "  p_12: {'min_impurity_decrease': 0.1},\n",
       "  p_15: {'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2019': {source: {'min_impurity_decrease': 0},\n",
       "  p_4: {'min_impurity_decrease': 0.01},\n",
       "  p_9: {'min_impurity_decrease': 0.01}},\n",
       " 'BPI Challenge 2012': {p_5: {'min_impurity_decrease': 0.01},\n",
       "  p_7: {'min_impurity_decrease': 0},\n",
       "  p_4: {'min_impurity_decrease': 0.01},\n",
       "  p_6: {'min_impurity_decrease': 0.01}},\n",
       " 'BPI Challenge 2017': {p_5: {'min_impurity_decrease': 0},\n",
       "  p_4: {'min_impurity_decrease': 0.01},\n",
       "  p_6: {'min_impurity_decrease': 0.01}}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_7: {'C': 0.25, 'tol': 0.001},\n",
       "  p_14: {'C': 0.1, 'tol': 0.001},\n",
       "  p_3: {'C': 0.5, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001},\n",
       "  p_9: {'C': 0.1, 'tol': 0.001},\n",
       "  p_12: {'C': 0.1, 'tol': 0.001},\n",
       "  p_15: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2019': {source: {'C': 0.5, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_9: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2012': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_7: {'C': 0.25, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2017': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001}}}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'C': 0.1, 'tol': 0.001},\n",
       "  p_7: {'C': 0.1, 'tol': 0.001},\n",
       "  p_14: {'C': 0.1, 'tol': 0.001},\n",
       "  p_3: {'C': 0.1, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.5, 'tol': 0.001},\n",
       "  p_9: {'C': 0.1, 'tol': 0.001},\n",
       "  p_12: {'C': 0.1, 'tol': 0.001},\n",
       "  p_15: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2019': {source: {'C': 0.5, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_9: {'C': 0.25, 'tol': 0.001}},\n",
       " 'BPI Challenge 2012': {p_5: {'C': 0.5, 'tol': 0.001},\n",
       "  p_7: {'C': 0.5, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001}},\n",
       " 'BPI Challenge 2017': {p_5: {'C': 0.25, 'tol': 0.001},\n",
       "  p_4: {'C': 0.1, 'tol': 0.001},\n",
       "  p_6: {'C': 0.1, 'tol': 0.001}}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'hidden_layer_sizes': (5, 5),\n",
       "   'learning_rate': 'adaptive'},\n",
       "  p_7: {'hidden_layer_sizes': (10, 10), 'learning_rate': 'invscaling'},\n",
       "  p_14: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'},\n",
       "  p_3: {'hidden_layer_sizes': (10, 10), 'learning_rate': 'adaptive'},\n",
       "  p_4: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'},\n",
       "  p_6: {'hidden_layer_sizes': (10, 10), 'learning_rate': 'invscaling'},\n",
       "  p_9: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'},\n",
       "  p_12: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'},\n",
       "  p_15: {'hidden_layer_sizes': (10, 10), 'learning_rate': 'constant'}},\n",
       " 'BPI Challenge 2019': {source: {'hidden_layer_sizes': (10, 10),\n",
       "   'learning_rate': 'constant'},\n",
       "  p_4: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'},\n",
       "  p_9: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}},\n",
       " 'BPI Challenge 2012': {p_5: {'hidden_layer_sizes': (5, 5),\n",
       "   'learning_rate': 'invscaling'},\n",
       "  p_7: {'hidden_layer_sizes': (10, 10), 'learning_rate': 'invscaling'},\n",
       "  p_4: {'hidden_layer_sizes': (5,), 'learning_rate': 'invscaling'},\n",
       "  p_6: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'}},\n",
       " 'BPI Challenge 2017': {p_5: {'hidden_layer_sizes': (5, 5),\n",
       "   'learning_rate': 'invscaling'},\n",
       "  p_4: {'hidden_layer_sizes': (5,), 'learning_rate': 'constant'},\n",
       "  p_6: {'hidden_layer_sizes': (5,), 'learning_rate': 'adaptive'}}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'max_depth': 4,\n",
       "   'min_impurity_decrease': 0},\n",
       "  p_7: {'max_depth': 2, 'min_impurity_decrease': 0},\n",
       "  p_14: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_3: {'max_depth': 2, 'min_impurity_decrease': 0.01},\n",
       "  p_4: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_6: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_9: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_12: {'max_depth': 1, 'min_impurity_decrease': 0.05},\n",
       "  p_15: {'max_depth': 1, 'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2019': {source: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_4: {'max_depth': 3, 'min_impurity_decrease': 0},\n",
       "  p_9: {'max_depth': 6, 'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2012': {p_5: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_7: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_4: {'max_depth': 6, 'min_impurity_decrease': 0},\n",
       "  p_6: {'max_depth': 1, 'min_impurity_decrease': 0}},\n",
       " 'BPI Challenge 2017': {p_5: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_4: {'max_depth': 1, 'min_impurity_decrease': 0},\n",
       "  p_6: {'max_depth': 1, 'min_impurity_decrease': 0}}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road Traffic Fine Management Process': {p_5: {'max_depth': 2,\n",
       "   'n_estimators': 100},\n",
       "  p_7: {'max_depth': 1, 'n_estimators': 100},\n",
       "  p_14: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_3: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_6: {'max_depth': 1, 'n_estimators': 100},\n",
       "  p_9: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_12: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_15: {'max_depth': 1, 'n_estimators': 20}},\n",
       " 'BPI Challenge 2019': {source: {'max_depth': 6, 'n_estimators': 150},\n",
       "  p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_9: {'max_depth': 2, 'n_estimators': 100}},\n",
       " 'BPI Challenge 2012': {p_5: {'max_depth': 2, 'n_estimators': 20},\n",
       "  p_7: {'max_depth': 2, 'n_estimators': 20},\n",
       "  p_4: {'max_depth': 4, 'n_estimators': 100},\n",
       "  p_6: {'max_depth': 1, 'n_estimators': 20}},\n",
       " 'BPI Challenge 2017': {p_5: {'max_depth': 6, 'n_estimators': 150},\n",
       "  p_4: {'max_depth': 1, 'n_estimators': 20},\n",
       "  p_6: {'max_depth': 6, 'n_estimators': 150}}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_values_xgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f8191b37d32cf9566ecd7cf3defa5afaec30f8fbee9c642d199bff281c069ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
