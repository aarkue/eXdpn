{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Analysis for Event Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "best_values_dt =  {}\n",
    "best_values_lr =  {}\n",
    "best_values_svm =  {}\n",
    "best_values_nn =  {}\n",
    "best_values_rf =  {}\n",
    "best_values_xgb =  {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Road Traffic Fine Management Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56f0f0561e74eae8dea5f6fb9da2cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/150370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('Road_Traffic_Fine_Management_Process.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2013-06-18 00:00:00+02:00\n",
      "First Timestamp: 2000-01-01 00:00:00+01:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp = max(event[\"time:timestamp\"] for trace in log_all for event in trace) \n",
    "print(\"Last Timestamp:\", last_timestamp)\n",
    "first_timestamp = min(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp) \n",
    "\n",
    "time_filtered_log = pm4py.filter_time_range(log_all, \"2012-01-01 00:00:00\", \"2013-06-18 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Fine', 'Send Fine', 'Insert Fine Notification', 'Add penalty', 'Send for Credit Collection', 'Payment', 'Insert Date Appeal to Prefecture', 'Send Appeal to Prefecture', 'Receive Result Appeal from Prefecture', 'Appeal to Judge', 'Notify Result Appeal to Offender']\n",
      "Number of different trace variants:  40\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Fine': 5558}\n",
      "{'Send Fine': 1351, 'Send for Credit Collection': 169, 'Payment': 3719, 'Receive Result Appeal from Prefecture': 10, 'Send Appeal to Prefecture': 283, 'Notify Result Appeal to Offender': 26}\n"
     ]
    }
   ],
   "source": [
    "log_start = pm4py.get_start_activities(time_filtered_log)\n",
    "print(log_start)\n",
    "\n",
    "end_activities = pm4py.get_end_activities(time_filtered_log)\n",
    "print(end_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcf71be34c947299fa977e15466f8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log = pm4py.filter_variants_top_k(time_filtered_log, 10)\n",
    "pm4py.write_xes(log, \"Road_Traffic_Fine_Management_Process_filtered.xes\")\n",
    "len(log) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e86344781094ea1818f48fbff62d0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/5485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes(\"Road_Traffic_Fine_Management_Process_filtered.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\")\n",
    "numeric_attributes = [\"amount\", \"expense\", \"totalPaymentAmount\", \"points\"]\n",
    "trace_attrs = [attr for attr in trace_attrs if max(list(pm4py.get_trace_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5834\t {'min_impurity_decrease': 0.01}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'min_impurity_decrease': 0.01}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8089\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.9972\t {'min_impurity_decrease': 0.01}\n",
      "p_6:\t0.9441\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.9322\t {'min_impurity_decrease': 0.1}\n",
      "final value: 0.020129932243921882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    X_train, y_train = [], []\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_['min_impurity_decrease']\n",
    "\n",
    "min_impurity_decrease = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {min_impurity_decrease}\")\n",
    "\n",
    "best_values_dt[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.6544\t {'hidden_layer_sizes': (10, 20, 10)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'hidden_layer_sizes': (10, 20, 10)}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'hidden_layer_sizes': (5,)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_14:\t0.9486\t {'hidden_layer_sizes': (5,)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_15:\t1.0\t {'hidden_layer_sizes': (10, 10)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8077\t {'hidden_layer_sizes': (10, 10)}\n",
      "p_4:\t0.9972\t {'hidden_layer_sizes': (5,)}\n",
      "p_6:\t0.9491\t {'hidden_layer_sizes': (10, 10)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8906\t {'hidden_layer_sizes': (5,)}\n",
      "final value: (5,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_['hidden_layer_sizes']\n",
    "\n",
    "hidden_layer_sizes = best_values[max_ds_key]\n",
    "print(f\"final value: {hidden_layer_sizes}\")\n",
    "\n",
    "best_values_nn[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5448\t {'C': 0.5}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'C': 0.25}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.4969\t {'C': 0.1}\n",
      "p_4:\t0.9972\t {'C': 0.1}\n",
      "p_6:\t0.9448\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'C': 0.1}\n",
      "final value: 0.19298126743722596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_['C']\n",
    "\n",
    "C_lr = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_lr}\")\n",
    "\n",
    "best_values_lr[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5448\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'C': 0.1}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.4969\t {'C': 0.1}\n",
      "p_4:\t0.9972\t {'C': 0.1}\n",
      "p_6:\t0.9444\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'C': 0.1}\n",
      "final value: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_['C']\n",
    "\n",
    "C_svm = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_svm}\")\n",
    "\n",
    "best_values_svm[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5559\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'max_depth': 1}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8081\t {'max_depth': 2}\n",
      "p_4:\t0.9972\t {'max_depth': 1}\n",
      "p_6:\t0.9474\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'max_depth': 1}\n",
      "final value: 1\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_xgb = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_xgb}\")\n",
    "\n",
    "best_values_xgb[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5588\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'n_estimators': 10}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9541\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8089\t {'n_estimators': 10}\n",
      "p_4:\t0.9972\t {'n_estimators': 10}\n",
      "p_6:\t0.9442\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'n_estimators': 10}\n",
      "final value: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_['n_estimators']\n",
    "\n",
    "n_estimators = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {n_estimators}\")\n",
    "\n",
    "best_values_rf[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2012 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a4ab417ad6471ab0fc08f764a48c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('financial_log.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED', 'O_ACCEPTED', 'O_CANCELLED', 'O_CREATED', 'O_DECLINED', 'O_SELECTED', 'O_SENT', 'O_SENT_BACK', 'W_Afhandelen leads', 'W_Beoordelen fraude', 'W_Completeren aanvraag', 'W_Nabellen incomplete dossiers', 'W_Nabellen offertes', 'W_Valideren aanvraag', 'W_Wijzigen contractgegevens']\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {sorted(list(pm4py.get_event_attribute_values(log_all,\"concept:name\").keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at subtraces of activities starting with an 'A'.\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    log_all,\n",
    "    \"concept:name\",\n",
    "    ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED'],\n",
    "    level=\"event\",\n",
    "    retain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d38b73ab53f4f29bde5d238d2a1e010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pm4py.write_xes(log, \"BPI Challenge 2012 only A.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfb8539dc1c45f19d1ca89889877a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/13087 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes('BPI Challenge 2012 only A.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "trace_attrs.remove(\"REG_DATE\")\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"concept:name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())\n",
    "total_size = sum(len(dp_dataset_map[key]) for key in dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\paper_data\\quantitative_analysis\\hyperparameter_analysis.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000041?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m dp \u001b[39min\u001b[39;00m dps:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000041?line=6'>7</a>\u001b[0m     dp_dataset \u001b[39m=\u001b[39m dp_dataset_map[dp]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000041?line=7'>8</a>\u001b[0m     X_train, y_train \u001b[39m=\u001b[39m basic_data_preprocessing(dp_dataset, numeric_attributes\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mAMOUNT_REQ\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000041?line=8'>9</a>\u001b[0m     ohe \u001b[39m=\u001b[39m fit_ohe(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000041?line=9'>10</a>\u001b[0m     X_train \u001b[39m=\u001b[39m apply_ohe(X_train, ohe)\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\exdpn\\data_preprocessing\\data_preprocessing.py:74\u001b[0m, in \u001b[0;36mbasic_data_preprocessing\u001b[1;34m(dataframe, impute, numeric_attributes)\u001b[0m\n\u001b[0;32m     72\u001b[0m target_var \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m df_X \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 74\u001b[0m df_X \u001b[39m=\u001b[39m df_X\u001b[39m.\u001b[39;49mdrop(target_var, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     75\u001b[0m df_y \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     76\u001b[0m df_y \u001b[39m=\u001b[39m dataframe[target_var]\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4906\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4774\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4775\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4776\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4783\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4784\u001b[0m ):\n\u001b[0;32m   4785\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4786\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4787\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4904\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4905\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4907\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4908\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4909\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4910\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4911\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4912\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4913\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4914\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4149\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4150\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4152\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4185\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4185\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4186\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{axis_name: new_axis})\n\u001b[0;32m   4188\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4189\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6017\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6016\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6017\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6018\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6019\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\paper_data\\quantitative_analysis\\hyperparameter_analysis.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000040?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m dp \u001b[39min\u001b[39;00m dps:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000040?line=6'>7</a>\u001b[0m     dp_dataset \u001b[39m=\u001b[39m dp_dataset_map[dp]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000040?line=7'>8</a>\u001b[0m     X_train, y_train \u001b[39m=\u001b[39m basic_data_preprocessing(dp_dataset, numeric_attributes\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mAMOUNT_REQ\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000040?line=8'>9</a>\u001b[0m     scaler, scaler_columns \u001b[39m=\u001b[39m fit_scaling(X_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mara%20Tews/Documents/Uni/RWTH_DataScience/SS_2022/SS2022_ProcessDiscoveryUsingPython/paper_data/quantitative_analysis/hyperparameter_analysis.ipynb#ch0000040?line=9'>10</a>\u001b[0m     X_train \u001b[39m=\u001b[39m apply_scaling(X_train, scaler, scaler_columns)\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\exdpn\\data_preprocessing\\data_preprocessing.py:74\u001b[0m, in \u001b[0;36mbasic_data_preprocessing\u001b[1;34m(dataframe, impute, numeric_attributes)\u001b[0m\n\u001b[0;32m     72\u001b[0m target_var \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m df_X \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 74\u001b[0m df_X \u001b[39m=\u001b[39m df_X\u001b[39m.\u001b[39;49mdrop(target_var, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     75\u001b[0m df_y \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     76\u001b[0m df_y \u001b[39m=\u001b[39m dataframe[target_var]\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4906\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4774\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   4775\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   4776\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4783\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4784\u001b[0m ):\n\u001b[0;32m   4785\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4786\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4787\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4904\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4905\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4906\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   4907\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   4908\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   4909\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   4910\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   4911\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   4912\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   4913\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   4914\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4150\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4148\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4149\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4150\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4152\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4185\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   4183\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4184\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4185\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4186\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{axis_name: new_axis})\n\u001b[0;32m   4188\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4189\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6017\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6015\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6016\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6017\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6018\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6019\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "\n",
    "    best_values[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted', cv=2)\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values[max_ds_key][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted', cv=2)\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted', cv=2)\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0225b3aa908147e9a283014aeac103b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/251734 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI_Challenge_2019.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2020-04-09 21:59:00+00:00\n",
      "First Timestamp: 1948-01-26 22:59:00+00:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp = max(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"Last Timestamp:\", last_timestamp)\n",
    "first_timestamp = min(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp)\n",
    "\n",
    "time_filtered_log = pm4py.filter_time_range(log_all, \"2018-09-01 00:00:00\", \"2018-12-01 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Purchase Order Item', 'Delete Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', 'Clear Invoice', 'Remove Payment Block', 'Cancel Goods Receipt', 'Change Quantity', 'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Price', 'Receive Order Confirmation', 'Change Storage Location', 'Change Delivery Indicator', 'Block Purchase Order Item', 'Create Purchase Requisition Item', 'Reactivate Purchase Order Item', 'Record Service Entry Sheet', 'SRM: Created', 'SRM: Complete', 'SRM: Awaiting Approval', 'SRM: Document Completed', 'SRM: Ordered', 'SRM: In Transfer to Execution Syst.', 'SRM: Change was Transmitted', 'SRM: Deleted', 'SRM: Transaction Completed', 'Cancel Subsequent Invoice', 'Change Approval for Purchase Order', 'Release Purchase Order', 'Update Order Confirmation', 'Record Subsequent Invoice', 'Change payment term', 'Change Final Invoice Indicator', 'Set Payment Block']\n",
      "Number of different trace variants:  1579\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different trace variants - subtraces:  1535\n"
     ]
    }
   ],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    time_filtered_log,\n",
    "    \"concept:name\",\n",
    "    ['Create Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', \n",
    "    'Clear Invoice', 'Record Service Entry Sheet', 'Cancel Goods Receipt', \n",
    "    'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Delivery Indicator', 'Remove Payment Block', \n",
    "    'Change Price', 'Delete Purchase Order Item', 'Change Quantity', \n",
    "    'Change Final Invoice Indicator', 'Receive Order Confirmation', 'Cancel Subsequent Invoice', \n",
    "    'Reactivate Purchase Order Item', 'Update Order Confirmation', 'Block Purchase Order Item', \n",
    "    'Change Approval for Purchase Order', 'Release Purchase Order', 'Record Subsequent Invoice', 'Set Payment Block', \n",
    "    'Create Purchase Requisition Item', 'Change Storage Location', 'Change Currency', 'Change payment term', \n",
    "    'Change Rejection Indicator', 'Release Purchase Requisition'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Purchase Order Item': 11176, 'Vendor creates invoice': 347, 'Vendor creates debit memo': 3, 'Create Purchase Requisition Item': 14665, 'Release Purchase Order': 1, 'Change Approval for Purchase Order': 13}\n",
      "Number of different trace variants - subtraces:  959\n",
      "{'Delete Purchase Order Item': 1850, 'Clear Invoice': 10521, 'Create Purchase Order Item': 1521, 'Cancel Goods Receipt': 175, 'Record Goods Receipt': 4576, 'Record Invoice Receipt': 5316, 'Receive Order Confirmation': 123, 'Change Delivery Indicator': 104, 'Block Purchase Order Item': 17, 'Record Service Entry Sheet': 401, 'Change Approval for Purchase Order': 211, 'Change Quantity': 107, 'Change Storage Location': 15, 'Change Price': 48, 'Remove Payment Block': 1187, 'Release Purchase Order': 1, 'Change payment term': 1, 'Cancel Subsequent Invoice': 4, 'Cancel Invoice Receipt': 1, 'Record Subsequent Invoice': 1, 'Vendor creates invoice': 23, 'Update Order Confirmation': 1, 'Set Payment Block': 1}\n",
      "Number of different trace variants - subtraces:  506\n"
     ]
    }
   ],
   "source": [
    "log_start = pm4py.get_start_activities(log)\n",
    "print(log_start)\n",
    "filtered_log = pm4py.filter_start_activities(log, 'Create Purchase Order Item')\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))\n",
    "\n",
    "end_activities = pm4py.get_end_activities(log)\n",
    "print(end_activities)\n",
    "filtered_log = pm4py.filter_end_activities(log, [\"Clear Invoice\"])\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306dee7ab0dd49dfb92d2d97e460516e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "exporting log, completed traces ::   0%|          | 0/8345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8345"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log = pm4py.filter_variants_top_k(filtered_log, 10)\n",
    "\n",
    "pm4py.write_xes(log, \"BPI_Challenge_2019_filtered_top_k.xes\")\n",
    "len(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2c2be80a6143778c90193b0397bee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/8345 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes(\"BPI_Challenge_2019_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\")\n",
    "event_attrs.remove(\"User\")\n",
    "trace_attrs = [attr for attr in trace_attrs if max(list(pm4py.get_trace_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "trace_attrs.remove(\"Name\")\n",
    "trace_attrs.remove(\"Item\")\n",
    "trace_attrs.remove(\"Purchasing Document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t0.8614\t {'min_impurity_decrease': 0}\n",
      "p_4:\t0.8753\t {'min_impurity_decrease': 0.01}\n",
      "p_9:\t0.9952\t {'min_impurity_decrease': 0.01}\n",
      "final value: 0.006666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_['min_impurity_decrease']\n",
    "\n",
    "min_impurity_decrease = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {min_impurity_decrease}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validaion Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_['hidden_layer_sizes']\n",
    "\n",
    "hidden_layer_sizes = best_values[max_ds_key]\n",
    "print(f\"final value: {hidden_layer_sizes}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_['C']\n",
    "\n",
    "C_lr = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_lr}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_['C']\n",
    "\n",
    "C_svm = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_svm}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "# data sets contain special characters which xgb boost cannot handel\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "best_values = {}\n",
    "for dp in dps: \n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    # remove special characters\n",
    "    # solution from: https://stackoverflow.com/questions/48645846/pythons-xgoost-valueerrorfeature-names-may-not-contain-or\n",
    "    X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_xgb = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_xgb}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_['n_estimators']\n",
    "\n",
    "n_estimators = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {n_estimators}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI Challenge 2017.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(log_all,\"concept:name\").keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at subtraces of activities starting with an 'O'.\n",
    "# Semantically, this means we look at the events corresponding to\n",
    "# the offer of a trace.\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    log_all,\n",
    "    \"concept:name\",\n",
    "    ['O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Returned',\n",
    "     'O_Accepted', 'O_Cancelled', 'O_Refused', 'O_Sent (online only)'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log)))\n",
    "\n",
    "filtered_log = pm4py.filter_variants(log, [\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (online only)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Refused')])\n",
    "\n",
    "print(\"Number of different trace variants - filtered subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))\n",
    "\n",
    "pm4py.write_xes(filtered_log, \"BPIChallenge2017_filtered.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases\n",
    "#log_top_k = pm4py.filter_variants_top_k(filtered_log, 5)\n",
    "#print(\"\"Number of different trace variants - filtered subtraces top k:\", len(pm4py.get_variants_as_tuples(log_top_200)))\n",
    "#pm4py.write_xes(log_top_k, \"BPIChallenge2017_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes('BPIChallenge2017_filtered.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_['min_impurity_decrease']\n",
    "\n",
    "min_impurity_decrease = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {min_impurity_decrease}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_['C']\n",
    "\n",
    "C_svm = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_svm}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_['hidden_layer_sizes']\n",
    "\n",
    "hidden_layer_sizes = best_values[max_ds_key]\n",
    "print(f\"final value: {hidden_layer_sizes}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_['C']\n",
    "\n",
    "C_lr = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_lr}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_xgb = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_xgb}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_rf = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_rf}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_xgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f8191b37d32cf9566ecd7cf3defa5afaec30f8fbee9c642d199bff281c069ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
