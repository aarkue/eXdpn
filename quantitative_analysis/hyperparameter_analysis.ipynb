{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Analysis for Event Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "best_values_dt =  {}\n",
    "best_values_lr =  {}\n",
    "best_values_svm =  {}\n",
    "best_values_nn =  {}\n",
    "best_values_rf =  {}\n",
    "best_values_xgb =  {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Road Traffic Fine Management Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mara Tews\\Documents\\Uni\\RWTH_DataScience\\SS_2022\\SS2022_ProcessDiscoveryUsingPython\\eXdpn\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "parsing log, completed traces :: 100%|██████████| 150370/150370 [01:19<00:00, 1884.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('Road_Traffic_Fine_Management_Process.xes.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Timestamp: 2013-06-18 00:00:00+02:00\n",
      "First Timestamp: 2000-01-01 00:00:00+01:00\n"
     ]
    }
   ],
   "source": [
    "last_timestamp = max(event[\"time:timestamp\"] for trace in log_all for event in trace) \n",
    "print(\"Last Timestamp:\", last_timestamp)\n",
    "first_timestamp = min(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp) \n",
    "\n",
    "time_filtered_log = pm4py.filter_time_range(log_all, \"2012-01-01 00:00:00\", \"2013-06-18 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities: ['Create Fine', 'Send Fine', 'Insert Fine Notification', 'Add penalty', 'Send for Credit Collection', 'Payment', 'Insert Date Appeal to Prefecture', 'Send Appeal to Prefecture', 'Receive Result Appeal from Prefecture', 'Appeal to Judge', 'Notify Result Appeal to Offender']\n",
      "Number of different trace variants:  40\n"
     ]
    }
   ],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Create Fine': 5558}\n",
      "{'Send Fine': 1351, 'Send for Credit Collection': 169, 'Payment': 3719, 'Receive Result Appeal from Prefecture': 10, 'Send Appeal to Prefecture': 283, 'Notify Result Appeal to Offender': 26}\n"
     ]
    }
   ],
   "source": [
    "log_start = pm4py.get_start_activities(time_filtered_log)\n",
    "print(log_start)\n",
    "\n",
    "end_activities = pm4py.get_end_activities(time_filtered_log)\n",
    "print(end_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exporting log, completed traces :: 100%|██████████| 5485/5485 [00:01<00:00, 3624.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5485"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log = pm4py.filter_variants_top_k(time_filtered_log, 10)\n",
    "pm4py.write_xes(log, \"Road_Traffic_Fine_Management_Process_filtered.xes\")\n",
    "len(log) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 5485/5485 [00:06<00:00, 885.80it/s] \n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes(\"Road_Traffic_Fine_Management_Process_filtered.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\")\n",
    "numeric_attributes = [\"amount\", \"expense\", \"totalPaymentAmount\", \"points\"]\n",
    "trace_attrs = [attr for attr in trace_attrs if max(list(pm4py.get_trace_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5834\t {'min_impurity_decrease': 0.01}\n",
      "p_4:\t0.9972\t {'min_impurity_decrease': 0.01}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'min_impurity_decrease': 0.01}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.9322\t {'min_impurity_decrease': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'min_impurity_decrease': 0}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8089\t {'min_impurity_decrease': 0}\n",
      "p_6:\t0.9439\t {'min_impurity_decrease': 0}\n",
      "final value: 0.020129932243921882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    X_train, y_train = [], []\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_['min_impurity_decrease']\n",
    "\n",
    "min_impurity_decrease = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {min_impurity_decrease}\")\n",
    "\n",
    "best_values_dt[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_3:\t0.6115\t {'hidden_layer_sizes': (5, 10, 5)}\n",
      "p_4:\t0.9972\t {'hidden_layer_sizes': (10, 20, 10)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t0.9951\t {'hidden_layer_sizes': (10, 10)}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'hidden_layer_sizes': (5,)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8951\t {'hidden_layer_sizes': (5,)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_14:\t0.9486\t {'hidden_layer_sizes': (5,)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'hidden_layer_sizes': (10, 10)}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_5:\t0.8077\t {'hidden_layer_sizes': (5, 5)}\n",
      "p_6:\t0.9483\t {'hidden_layer_sizes': (10, 20, 10)}\n",
      "final value: (10, 20, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10), (5, 10, 5), (10, 20, 10))}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_['hidden_layer_sizes']\n",
    "\n",
    "hidden_layer_sizes = best_values[max_ds_key]\n",
    "print(f\"final value: {hidden_layer_sizes}\")\n",
    "\n",
    "best_values_nn[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5448\t {'C': 0.5}\n",
      "p_4:\t0.9972\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'C': 0.25}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.4969\t {'C': 0.1}\n",
      "p_6:\t0.9448\t {'C': 0.1}\n",
      "final value: 0.19298126743722596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_['C']\n",
    "\n",
    "C_lr = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_lr}\")\n",
    "\n",
    "best_values_lr[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5448\t {'C': 0.1}\n",
      "p_4:\t0.9972\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'C': 0.1}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'C': 0.1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.4969\t {'C': 0.1}\n",
      "p_6:\t0.9444\t {'C': 0.1}\n",
      "final value: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_['C']\n",
    "\n",
    "C_svm = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_svm}\")\n",
    "\n",
    "best_values_svm[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5559\t {'max_depth': 1}\n",
      "p_4:\t0.9972\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'max_depth': 1}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9563\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'max_depth': 1}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8081\t {'max_depth': 2}\n",
      "p_6:\t0.9474\t {'max_depth': 1}\n",
      "final value: 1\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 5, 10)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_xgb = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_xgb}\")\n",
    "\n",
    "best_values_xgb[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "p_3:\t0.5502\t {'n_estimators': 100}\n",
      "p_4:\t0.9949\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_7:\t1.0\t {'n_estimators': 10}\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_9:\t0.9552\t {'n_estimators': 100}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_12:\t0.8904\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_14:\t0.9486\t {'n_estimators': 10}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::expense` and `case::expense` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_15:\t1.0\t {'n_estimators': 20}\n",
      "Warning: Key `event::amount` and `case::amount` was not present.\n",
      "Warning: Key `event::totalPaymentAmount` and `case::totalPaymentAmount` was not present.\n",
      "Warning: Key `event::points` and `case::points` was not present.\n",
      "p_5:\t0.8089\t {'n_estimators': 10}\n",
      "p_6:\t0.9468\t {'n_estimators': 10}\n",
      "final value: 33\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators': (10, 20, 50, 100)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute = True, numeric_attributes = numeric_attributes)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_['n_estimators']\n",
    "\n",
    "n_estimators = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {n_estimators}\")\n",
    "\n",
    "best_values_rf[\"Road Traffic Fine Management Process\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2012 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI_Challenge_2012.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'activities: {sorted(list(pm4py.get_event_attribute_values(log_all,\"concept:name\").keys()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at subtraces of activities starting with an 'A'.\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    log_all,\n",
    "    \"concept:name\",\n",
    "    ['A_ACCEPTED', 'A_ACTIVATED', 'A_APPROVED', 'A_CANCELLED', 'A_DECLINED', 'A_FINALIZED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'A_REGISTERED', 'A_SUBMITTED'],\n",
    "    level=\"event\",\n",
    "    retain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm4py.write_xes(log, \"BPI Challenge 2012 only A.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes('BPI Challenge 2012 only A.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "trace_attrs.remove(\"REG_DATE\")\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"concept:name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())\n",
    "total_size = sum(len(dp_dataset_map[key]) for key in dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_\n",
    "\n",
    "dt_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {dt_param}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "\n",
    "    best_values[dp] = svm_grid.best_params_\n",
    "\n",
    "svm_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {svm_param}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10)), 'learning_rate': ('constant', 'invscaling', 'adaptive')}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted', cv=2)\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_\n",
    "\n",
    "nn_param = {param: best_values[max_ds_key][param] for param in parameters.keys()}\n",
    "print(f\"final value: {nn_param}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5), 'tol': (0.001, 0.0005, 0.0015)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted', cv=2)\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_\n",
    "\n",
    "lr_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "print(f\"final value: {lr_param}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'n_estimators': (20,100,150)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted', cv=2)\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_\n",
    "\n",
    "xgb_param = {param: round(sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size) for param in parameters.keys()}\n",
    "print(f\"final value: {xgb_param}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6), 'min_impurity_decrease': (0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, numeric_attributes=[\"AMOUNT_REQ\"])\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_\n",
    "\n",
    "rf_param = {param: sum(val[param]*len(dp_dataset_map[key]) for key,val in best_values.items())/total_size for param in parameters.keys()}\n",
    "rf_param['max_depth'] = round(rf_param['max_depth'])\n",
    "print(f\"final value: {rf_param}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2012\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI_Challenge_2019.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_timestamp = max(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"Last Timestamp:\", last_timestamp)\n",
    "first_timestamp = min(event[\"time:timestamp\"] for trace in log_all for event in trace)\n",
    "print(\"First Timestamp:\", first_timestamp)\n",
    "\n",
    "time_filtered_log = pm4py.filter_time_range(log_all, \"2018-09-01 00:00:00\", \"2018-12-01 00:00:00\", mode='traces_contained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(time_filtered_log,\"concept:name\").keys())}')\n",
    "print(\"Number of different trace variants: \", len(pm4py.get_variants_as_tuples(time_filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    time_filtered_log,\n",
    "    \"concept:name\",\n",
    "    ['Create Purchase Order Item', 'Vendor creates invoice', 'Record Goods Receipt', 'Record Invoice Receipt', \n",
    "    'Clear Invoice', 'Record Service Entry Sheet', 'Cancel Goods Receipt', \n",
    "    'Vendor creates debit memo', 'Cancel Invoice Receipt', 'Change Delivery Indicator', 'Remove Payment Block', \n",
    "    'Change Price', 'Delete Purchase Order Item', 'Change Quantity', \n",
    "    'Change Final Invoice Indicator', 'Receive Order Confirmation', 'Cancel Subsequent Invoice', \n",
    "    'Reactivate Purchase Order Item', 'Update Order Confirmation', 'Block Purchase Order Item', \n",
    "    'Change Approval for Purchase Order', 'Release Purchase Order', 'Record Subsequent Invoice', 'Set Payment Block', \n",
    "    'Create Purchase Requisition Item', 'Change Storage Location', 'Change Currency', 'Change payment term', \n",
    "    'Change Rejection Indicator', 'Release Purchase Requisition'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_start = pm4py.get_start_activities(log)\n",
    "print(log_start)\n",
    "filtered_log = pm4py.filter_start_activities(log, 'Create Purchase Order Item')\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))\n",
    "\n",
    "end_activities = pm4py.get_end_activities(log)\n",
    "print(end_activities)\n",
    "filtered_log = pm4py.filter_end_activities(log, [\"Clear Invoice\"])\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases.\n",
    "log = pm4py.filter_variants_top_k(filtered_log, 10)\n",
    "\n",
    "pm4py.write_xes(log, \"BPI_Challenge_2019_filtered_top_k.xes\")\n",
    "len(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes(\"BPI_Challenge_2019_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exdpn.petri_net import get_petri_net\n",
    "net, im, fm = get_petri_net(log, miner_type='IM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\")\n",
    "event_attrs.remove(\"User\")\n",
    "trace_attrs = [attr for attr in trace_attrs if max(list(pm4py.get_trace_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "trace_attrs.remove(\"Name\")\n",
    "trace_attrs.remove(\"Item\")\n",
    "trace_attrs.remove(\"Purchasing Document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "from exdpn.guards import ML_Technique\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree (with respect to explainablility):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_['min_impurity_decrease']\n",
    "\n",
    "min_impurity_decrease = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {min_impurity_decrease}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validaion Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10), (5, 10, 5))}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_['hidden_layer_sizes']\n",
    "\n",
    "hidden_layer_sizes = best_values[max_ds_key]\n",
    "print(f\"final value: {hidden_layer_sizes}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_['C']\n",
    "\n",
    "C_lr = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_lr}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_['C']\n",
    "\n",
    "C_svm = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_svm}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "parameters = {'max_depth': (1, 3, 5, 10)}\n",
    "\n",
    "# data sets contain special characters which xgb boost cannot handel\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "best_values = {}\n",
    "for dp in dps: \n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    # remove special characters\n",
    "    # solution from: https://stackoverflow.com/questions/48645846/pythons-xgoost-valueerrorfeature-names-may-not-contain-or\n",
    "    X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_xgb = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_xgb}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'n_estimators': (10, 20, 50)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset, impute=True)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_['n_estimators']\n",
    "\n",
    "n_estimators = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {n_estimators}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2019\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPI Challenge 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log_all = pm4py.read_xes('BPI Challenge 2017.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'activities: {list(pm4py.get_event_attribute_values(log_all,\"concept:name\").keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at subtraces of activities starting with an 'O'.\n",
    "# Semantically, this means we look at the events corresponding to\n",
    "# the offer of a trace.\n",
    "log = pm4py.filter_event_attribute_values(\n",
    "    log_all,\n",
    "    \"concept:name\",\n",
    "    ['O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Returned',\n",
    "     'O_Accepted', 'O_Cancelled', 'O_Refused', 'O_Sent (online only)'],\n",
    "    level=\"event\",\n",
    "    retain=True)\n",
    "\n",
    "print(\"Number of different trace variants - subtraces: \", len(pm4py.get_variants_as_tuples(log)))\n",
    "\n",
    "filtered_log = pm4py.filter_variants(log, [\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (mail and online)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (mail and online)', 'O_Returned', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Accepted'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Refused'),\n",
    "    ('O_Create Offer', 'O_Created',\n",
    "     'O_Sent (online only)', 'O_Returned', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Cancelled'),\n",
    "    ('O_Create Offer', 'O_Created', 'O_Sent (online only)', 'O_Returned', 'O_Refused')])\n",
    "\n",
    "print(\"Number of different trace variants - filtered subtraces: \", len(pm4py.get_variants_as_tuples(filtered_log)))\n",
    "\n",
    "pm4py.write_xes(filtered_log, \"BPIChallenge2017_filtered.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discard less frequent variants in order to reduce\n",
    "# the number of edge cases\n",
    "#log_top_k = pm4py.filter_variants_top_k(filtered_log, 5)\n",
    "#print(\"\"Number of different trace variants - filtered subtraces top k:\", len(pm4py.get_variants_as_tuples(log_top_200)))\n",
    "#pm4py.write_xes(log_top_k, \"BPIChallenge2017_filtered_top_k.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "log = pm4py.read_xes('BPIChallenge2017_filtered.xes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_attrs = list(pm4py.get_trace_attributes(log))\n",
    "event_attrs = list(pm4py.get_event_attributes(log))\n",
    "event_attrs = [attr for attr in event_attrs if max(list(pm4py.get_event_attribute_values(log, attr).values())) != 1 and \"ID\" not in attr]\n",
    "event_attrs.remove(\"time:timestamp\")\n",
    "event_attrs.remove(\"org:resource\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "from exdpn.petri_net import get_petri_net\n",
    "from exdpn.guard_datasets import extract_all_datasets\n",
    "from exdpn.data_preprocessing import basic_data_preprocessing\n",
    "from exdpn.data_preprocessing.data_preprocessing import apply_ohe, apply_scaling, fit_scaling, fit_ohe\n",
    "\n",
    "# prepare data\n",
    "dp_dataset_map = extract_all_datasets(log= log, net=net, initial_marking=im, final_marking=fm, \n",
    "                                      event_level_attributes = event_attrs,\n",
    "                                      case_level_attributes=trace_attrs)\n",
    "\n",
    "# decision points \n",
    "dps = list(dp_dataset_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Decision Tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that decision trees tend to be enormously large without any hyperparameters. We thus try to find an optimal `min_impurity_decrease` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "parameters = {'min_impurity_decrease':(0, 0.01, 0.05, 0.1, 0.15)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    dt_base = DecisionTreeClassifier()\n",
    "    dt_grid = GridSearchCV(dt_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    dt_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(dt_grid.best_score_,4)}\\t {dt_grid.best_params_}\")\n",
    "    best_values[dp] = dt_grid.best_params_['min_impurity_decrease']\n",
    "\n",
    "min_impurity_decrease = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {min_impurity_decrease}\")\n",
    "\n",
    "best_values_dt[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-Validation SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "parameters = {'C':(0.1, 0.25, 0.5)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    svm_base = LinearSVC()\n",
    "    svm_grid = GridSearchCV(svm_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    svm_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(svm_grid.best_score_,4)}\\t {svm_grid.best_params_}\")\n",
    "    best_values[dp] = svm_grid.best_params_['C']\n",
    "\n",
    "C_svm = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_svm}\")\n",
    "\n",
    "best_values_svm[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "parameters = {'hidden_layer_sizes': ((5, ), (5, 5), (10, 10), (5, 10, 5), (10, 20, 10))}\n",
    "\n",
    "best_values = {}\n",
    "max_ds_size = -1\n",
    "max_ds_key = None\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    if len(dp_dataset) > max_ds_size: max_ds_key = dp\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    nn_base = MLPClassifier()\n",
    "    nn_grid = GridSearchCV(nn_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    nn_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(nn_grid.best_score_,4)}\\t {nn_grid.best_params_}\")\n",
    "    best_values[dp] = nn_grid.best_params_['hidden_layer_sizes']\n",
    "\n",
    "hidden_layer_sizes = best_values[max_ds_key]\n",
    "print(f\"final value: {hidden_layer_sizes}\")\n",
    "\n",
    "best_values_nn[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters = {'C': (0.1, 0.25, 0.5)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    scaler, scaler_columns = fit_scaling(X_train)\n",
    "    X_train = apply_scaling(X_train, scaler, scaler_columns)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    lr_base = LogisticRegression()\n",
    "    lr_grid = GridSearchCV(lr_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    lr_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(lr_grid.best_score_,4)}\\t {lr_grid.best_params_}\")\n",
    "    best_values[dp] = lr_grid.best_params_['C']\n",
    "\n",
    "C_lr = sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys())\n",
    "print(f\"final value: {C_lr}\")\n",
    "\n",
    "best_values_lr[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    xgb_base = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    xgb_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(xgb_grid.best_score_,4)}\\t {xgb_grid.best_params_}\")\n",
    "    best_values[dp] = xgb_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_xgb = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_xgb}\")\n",
    "\n",
    "best_values_xgb[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-Validation Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parameters = {'max_depth': (1, 2, 3, 4, 6)}\n",
    "\n",
    "best_values = {}\n",
    "\n",
    "for dp in dps:\n",
    "    dp_dataset = dp_dataset_map[dp]\n",
    "    X_train, y_train = basic_data_preprocessing(dp_dataset)\n",
    "    ohe = fit_ohe(X_train)\n",
    "    X_train = apply_ohe(X_train, ohe)\n",
    "    transition_int_map = {transition: index for index,\n",
    "                          transition in enumerate(list(set(y_train)))}\n",
    "    y_train_mapped = [transition_int_map[transition] for transition in y_train]\n",
    "\n",
    "    rf_base = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf_base, parameters, n_jobs=-1, scoring='f1_weighted')\n",
    "\n",
    "    rf_grid.fit(X_train, y_train_mapped)\n",
    "    print(f\"{dp}:\\t{round(rf_grid.best_score_,4)}\\t {rf_grid.best_params_}\")\n",
    "    best_values[dp] = rf_grid.best_params_['max_depth']\n",
    "\n",
    "max_depth_rf = round(sum(val * len(dp_dataset_map[key]) for key,val in best_values.items())/sum(len(dp_dataset_map[key]) for key in best_values.keys()))\n",
    "print(f\"final value: {max_depth_rf}\")\n",
    "\n",
    "best_values_rf[\"BPI Challenge 2017\"] = best_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values_xgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f8191b37d32cf9566ecd7cf3defa5afaec30f8fbee9c642d199bff281c069ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
